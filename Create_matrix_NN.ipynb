{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b5f0f1-bddf-4fc7-a673-2d66cf30b979",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eb9a7d-0d3b-4a60-b036-6830dea53240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "  Using cached tables-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Requirement already satisfied: cython>=0.29.21 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from tables) (0.29.34)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from tables) (1.23.5)\n",
      "Collecting numexpr>=2.6.2 (from tables)\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Collecting blosc2~=2.0.0 (from tables)\n",
      "  Using cached blosc2-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.10/site-packages (from tables) (23.1)\n",
      "Collecting py-cpuinfo (from tables)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: msgpack in /srv/conda/envs/notebook/lib/python3.10/site-packages (from blosc2~=2.0.0->tables) (1.0.5)\n",
      "Installing collected packages: py-cpuinfo, numexpr, blosc2, tables\n",
      "Successfully installed blosc2-2.0.0 numexpr-2.8.4 py-cpuinfo-9.0.0 tables-3.8.0\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torchvision) (2.29.0)\n",
      "Collecting torch==2.0.1 (from torchvision)\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Collecting filelock (from torch==2.0.1->torchvision)\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
      "Collecting sympy (from torch==2.0.1->torchvision)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchvision)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->torchvision)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (67.7.2)\n",
      "Requirement already satisfied: wheel in /srv/conda/envs/notebook/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.40.0)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->torchvision)\n",
      "  Using cached cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->torchvision)\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.0.1->torchvision)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision\n",
      "Successfully installed cmake-3.27.0 filelock-3.12.2 lit-16.0.6 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tables\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefb40b8-d539-478b-a348-fe633afbc6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "False\n",
      "lat lon colums Index(['lat', 'lon', 'key_2', 'key_3', 'start_date', 'rhum_shift30',\n",
      "       'pres_shift30', 'GPP_shift30', 'RECO_shift30', 'Unnamed: 0_shift29',\n",
      "       'CASM_soil_moisture_shift29', 'rhum_shift60', 'pres_shift60',\n",
      "       'GPP_shift60', 'RECO_shift60', 'Unnamed: 0_shift58',\n",
      "       'CASM_soil_moisture_shift58', 'rhum_shift365', 'pres_shift365',\n",
      "       'GPP_shift365', 'RECO_shift365', 'Unnamed: 0_shift365',\n",
      "       'CASM_soil_moisture_shift365', 'ccsm3', 'nasa', 'nmme_mean',\n",
      "       'nmme_wo_ccsm3_nasa', 'ccsm3_0', 'nasa_0', 'nmme0_mean',\n",
      "       'nmme0_wo_ccsm3_nasa', 'ccsm3_shift15', 'nasa_shift15',\n",
      "       'nmme_mean_shift15', 'nmme_wo_ccsm3_nasa_shift15', 'ccsm3_0_shift15',\n",
      "       'nasa_0_shift15', 'nmme0_mean_shift15', 'nmme0_wo_ccsm3_nasa_shift15',\n",
      "       'tmp2m', 'tmp2m_sqd', 'tmp2m_std', 'tmp2m_clim', 'tmp2m_anom',\n",
      "       'precip_shift29', 'precip_shift29_clim', 'precip_shift29_anom',\n",
      "       'tmp2m_shift29', 'tmp2m_sqd_shift29', 'tmp2m_std_shift29',\n",
      "       'tmp2m_shift29_clim', 'tmp2m_shift29_anom', 'precip_shift58',\n",
      "       'precip_shift58_clim', 'precip_shift58_anom', 'tmp2m_shift58',\n",
      "       'tmp2m_sqd_shift58', 'tmp2m_std_shift58', 'tmp2m_shift58_clim',\n",
      "       'tmp2m_shift58_anom', 'precip_shift365', 'precip_shift365_clim',\n",
      "       'precip_shift365_anom', 'tmp2m_shift365', 'tmp2m_sqd_shift365',\n",
      "       'tmp2m_std_shift365', 'tmp2m_shift365_clim', 'tmp2m_shift365_anom',\n",
      "       'tmp2m_anom_inv_std'],\n",
      "      dtype='object')\n",
      "relevant_cols {'rhum_shift30', 'CASM_soil_moisture_shift29', 'wind_hgt_10_2010_2_shift30', 'GPP_shift30', 'pres_shift30', 'nmme_wo_ccsm3_nasa', 'sst_2010_3_shift30', 'nmme0_wo_ccsm3_nasa', 'precip', 'mei_shift45', 'RECO_shift30', 'tmp2m_clim', 'sst_2010_1_shift30', 'icec_2010_2_shift30', 'tmp2m_shift29', 'icec_2010_1_shift30', 'icec_2010_3_shift30', 'lat', 'lon', 'sst_2010_2_shift30', 'ones', 'start_date', 'tmp2m_anom', 'phase_shift17', 'wind_hgt_10_2010_1_shift30', 'zeros', 'tmp2m_shift58', 'tmp2m_shift29_anom', 'tmp2m_shift58_anom'}\n",
      "data colums Index(['lat', 'lon', 'start_date', 'rhum_shift30', 'pres_shift30',\n",
      "       'GPP_shift30', 'RECO_shift30', 'CASM_soil_moisture_shift29',\n",
      "       'nmme_wo_ccsm3_nasa', 'nmme0_wo_ccsm3_nasa', 'tmp2m_clim', 'tmp2m_anom',\n",
      "       'tmp2m_shift29', 'tmp2m_shift29_anom', 'tmp2m_shift58',\n",
      "       'tmp2m_shift58_anom', 'mei_shift45', 'phase_shift17',\n",
      "       'sst_2010_1_shift30', 'sst_2010_2_shift30', 'sst_2010_3_shift30',\n",
      "       'icec_2010_1_shift30', 'icec_2010_2_shift30', 'icec_2010_3_shift30',\n",
      "       'wind_hgt_10_2010_1_shift30', 'wind_hgt_10_2010_2_shift30'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=12)\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 12)\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# Ensure that working directory is forecast_rodeo\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    # Navigate to forecast_rodeo\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "if os.path.basename(os.getcwd()) != \"forecast_rodeo_upgrade\":\n",
    "    raise Exception(\"You must be in the forecast_rodeo_upgrade folder\")\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "\n",
    "# Experiment name\n",
    "experiment = \"neural_network\"\n",
    "\n",
    "#####\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import shap\n",
    "\n",
    "# Choose target\n",
    "#\n",
    "gt_id = \"contest_tmp2m\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "target_horizon = \"34w\" # \"34w\" or \"56w\"\n",
    "\n",
    "##CAUTION ! It seeems that for both contest_tmp2m 34w and contest precip 56w all the CASM GPP RECO have been added correctly. But apparently for contest_tmp2m 56w and \n",
    "\n",
    "# Create list of official contest submission dates in YYYYMMDD format\n",
    "#\n",
    "submission_dates = [datetime(y,4,18)+timedelta(14*i) for y in range(2011,2018) for i in range(26)]\n",
    "submission_dates = ['{}{:02d}{:02d}'.format(date.year, date.month, date.day) for date in submission_dates]\n",
    "submission_dates = [datetime.strptime(str(d), \"%Y%m%d\") for d in submission_dates]\n",
    "submission_dates = pd.Series(submission_dates)\n",
    "\n",
    "# Create list of target dates corresponding to submission dates in YYYYMMDD format\n",
    "#\n",
    "target_dates = pd.Series([get_target_date('{}{:02d}{:02d}'.format(date.year, date.month, date.day), target_horizon) for date in submission_dates])\n",
    "\n",
    "# Find all unique target day-month combinations\n",
    "target_day_months = pd.DataFrame({'month' : target_dates.dt.month, \n",
    "                                  'day': target_dates.dt.day}).drop_duplicates()\n",
    "\n",
    "#####\n",
    "\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "# Load functionality for stepwise regression\n",
    "from stepwise_util import *\n",
    "\n",
    "hindcast_features = False if len(sys.argv) < 6 else (sys.argv[5] == \"True\")\n",
    "print(hindcast_features)\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# anom_inv_std_col: column name of inverse standard deviation of anomalies for each start_date\n",
    "anom_inv_std_col = anom_col+\"_inv_std\"\n",
    "\n",
    "#\n",
    "# Default regression parameter values\n",
    "#\n",
    "# anom_scale_col: multiply anom_col by this amount prior to prediction\n",
    "# (e.g., 'ones' or anom_inv_std_col)\n",
    "anom_scale_col = 'ones'\n",
    "# pred_anom_scale_col: multiply predicted anomalies by this amount\n",
    "# (e.g., 'ones' or anom_inv_std_col)\n",
    "pred_anom_scale_col = 'ones'\n",
    "# choose first year to use in training set # (before 1979 = bad)\n",
    "first_train_year = 1979 if gt_id == 'contest_precip' else 1979\n",
    "# columns to group by when fitting regressions (a separate regression\n",
    "# is fit for each group); use ['ones'] to fit a single regression to all points\n",
    "group_by_cols = ['lat', 'lon']\n",
    "# base_col: column which should be subtracted from gt_col prior to prediction\n",
    "# (e.g., this might be clim_col or a baseline predictor like NMME)\n",
    "base_col = 'zeros'\n",
    "#\n",
    "# Default stepwise parameter values\n",
    "#\n",
    "# Define candidate predictors\n",
    "initial_candidate_x_cols = default_stepwise_candidate_predictors(gt_id, target_horizon, hindcast=hindcast_features)\n",
    "# Copy the list of candidates for later modification\n",
    "candidate_x_cols = initial_candidate_x_cols[:]\n",
    "# Skill threshold for what counts as a similar year\n",
    "similar_years_threshold = 0.1\n",
    "# Tolerance for convergence: if improvement is less than tolerance, terminate.\n",
    "tolerance = 0.01\n",
    "# Whether to use margin days (days around the target date)\n",
    "use_margin = False\n",
    "\n",
    "\n",
    "date_data = pd.read_hdf('results/regression/shared/' + gt_id + '_' + target_horizon + '/date_data-'+ gt_id + '_' + target_horizon + '.h5')\n",
    "lat_lon_date_data = pd.read_hdf('results/regression/shared/' + gt_id + '_' + target_horizon + '/lat_lon_date_data-'+ gt_id + '_' + target_horizon + '.h5')\n",
    "print('lat lon colums', lat_lon_date_data.columns)\n",
    "\n",
    "if target_horizon == \"34w\":\n",
    "    relevant_cols = set(candidate_x_cols+[base_col,clim_col,anom_col,'start_date','lat','lon','precip','GPP_shift30','RECO_shift30','CASM_soil_moisture_shift29']+group_by_cols)\n",
    "if target_horizon == \"56w\":\n",
    "    relevant_cols = set(candidate_x_cols+[base_col,clim_col,anom_col,'start_date','lat','lon','precip','GPP_shift44','RECO_shift44','CASM_soil_moisture_shift43']+group_by_cols)\n",
    "print('relevant_cols', relevant_cols)\n",
    "data = lat_lon_date_data.loc[lat_lon_date_data.start_date.dt.year >= first_train_year,\n",
    "                             lat_lon_date_data.columns.isin(relevant_cols)]\n",
    "data = pd.merge(data, date_data.loc[date_data.start_date.dt.year >= first_train_year,\n",
    "                                    date_data.columns.isin(relevant_cols)],\n",
    "                on=\"start_date\", how=\"left\")\n",
    "print('data colums', data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e607ab-725f-429d-b536-b898eb101f00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Process of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bedaf5-ae3a-443c-aa08-9b287dd11595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Restriction of the data through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ccfb93b-5ef3-4c63-826d-558e666b15f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          1979-01-01\n",
      "1          1979-01-02\n",
      "2          1979-01-03\n",
      "3          1979-01-04\n",
      "4          1979-01-05\n",
      "              ...    \n",
      "44285680   2018-05-07\n",
      "44285681   2018-05-10\n",
      "44285682   2018-05-13\n",
      "44285683   2018-05-16\n",
      "44285684   2018-05-22\n",
      "Name: start_date, Length: 38264313, dtype: datetime64[ns]\n",
      "(14388,)\n"
     ]
    }
   ],
   "source": [
    "# Restriction over the dates\n",
    "condition00 = (data['start_date'] >= '1979-01-01')\n",
    "condition01 = (data['start_date'] <= '2018-05-23')\n",
    "data = data.drop(data[~condition00].index)\n",
    "data = data.drop(data[~condition01].index)\n",
    "print(data['start_date'])\n",
    "\n",
    "# delete the duplicates to have a list of all the different existing dates\n",
    "extract_time = data['start_date'].drop_duplicates().sort_values() #\n",
    "print(extract_time.shape)\n",
    "             \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95409dba-d776-40c5-bdba-d4f2a43cbf59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restriction of the data through latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e39121d-358b-45a4-9017-1d6cf4c18032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.0 28.0 29.0 30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0 40.0\n",
      " 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0]\n",
      "[236.0 237.0 238.0 239.0 240.0 241.0 242.0 243.0 244.0 245.0 246.0 247.0\n",
      " 248.0 249.0 250.0 251.0 252.0 253.0 254.0 255.0 256.0 257.0 258.0 259.0\n",
      " 260.0 261.0 262.0 263.0 264.0 265.0 266.0]\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the latitude and longitudes values who are integers\n",
    "condition5 = data['lat'].astype(int) == data['lat']\n",
    "data = data.drop(data[~condition5].index)\n",
    "condition6 = data['lon'].astype(int) == data['lon']\n",
    "data = data.drop(data[~condition6].index)\n",
    "\n",
    "#restriction to the smallest rectangle possible\n",
    "condition1 = (data['lat'] < 27.0) | (data['lat'] > 49.0)\n",
    "condition2 = (data['lon'] < 236.0) | (data['lon'] > 266.0)\n",
    "data = data.drop(data[condition1].index)\n",
    "data = data.drop(data[condition2].index)\n",
    "\n",
    "#creates vectors containing all the possible values of latitude and longitude, dropping duplicates\n",
    "latitude_values = data['lat'].drop_duplicates().sort_values()\n",
    "longitude_values = data['lon'].drop_duplicates().sort_values()\n",
    "latitude_values = np.array(latitude_values)\n",
    "longitude_values = np.array(longitude_values)\n",
    "print(latitude_values)\n",
    "print(longitude_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effe6e6-6d58-4805-b313-0fdb071cf9ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creation and save of the Mask\n",
    "Can be done only one time, because it is a long step. A mask is already saved in results/matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841e004c-4d1b-4123-b282-a9af8504959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g (31, 23)\n",
      "gk (31, 23)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True False False False False False False  True False False  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  False False False False False False False False False False  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True  True False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False]\n",
      " [ True False False False False False False False False False False False\n",
      "  False False False False False False False False False  True  True]\n",
      " [ True  True False False False False False False False False False False\n",
      "  False False False  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True False False False False False False False False False\n",
      "  False  True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True False False  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True]]\n",
      "torch.Size([23, 31])\n",
      "Saving multiarrays features to results/matrix/mask_tensor_permuted_TEST.tensor\n",
      "Finished generating data matrix.\n"
     ]
    }
   ],
   "source": [
    "#Determine minimum and maximum latitude and longitude values\n",
    "min_latitude = np.min(latitude_values)\n",
    "max_latitude = np.max(latitude_values)\n",
    "min_longitude = np.min(longitude_values)\n",
    "max_longitude = np.max(longitude_values)\n",
    "\n",
    "#Create meshgrid of latitude and longitude values\n",
    "grid_latitude, grid_longitude = np.meshgrid(np.arange(min_latitude, max_latitude + 1), np.arange(min_longitude, max_longitude + 1))\n",
    "print('g',grid_latitude.shape)\n",
    "print('gk',grid_latitude.shape)\n",
    "# Check existence of grid points in the dataframe\n",
    "mask = np.zeros_like(grid_latitude, dtype=bool)\n",
    "for i in range(len(latitude_values)):\n",
    "    print(i)\n",
    "    lat_idx = np.where(grid_latitude == latitude_values[i])\n",
    "    for j in range(len(longitude_values)):\n",
    "        lon_idx = np.where(grid_longitude == longitude_values[j])\n",
    "        exists_in_data = (data['lat'] == latitude_values[i]) & (data['lon'] == longitude_values[j])\n",
    "        if exists_in_data.any():\n",
    "            mask[j, i] = True\n",
    "\n",
    "# Create the final mask\n",
    "final_mask = np.logical_not(mask)\n",
    "print(final_mask)\n",
    "\n",
    "final_mask = th.tensor(final_mask)\n",
    "final_mask = final_mask.permute(1,0)\n",
    "print(final_mask.shape)\n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('results', 'matrix')\n",
    "# e.g., cache_dir = 'results/regression/shared/contest_precip_34w'\n",
    "\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Filenames for data file to be stored in cache_dir\n",
    "data_file = os.path.join(\n",
    "    cache_dir, \"mask_tensor_permuted_TEST.tensor\")\n",
    "\n",
    "print(\"Saving multiarrays features to \" + data_file)\n",
    "th.save(final_mask, data_file)\n",
    "\n",
    "print(\"Finished generating data matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be7cc5-5635-4479-bad6-b0fbcca2d1d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Copy of the data in another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ac5a11a-87b5-4bfa-8d77-ddedac4b229c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns Index(['lat', 'lon', 'start_date', 'rhum_shift30', 'pres_shift30',\n",
      "       'GPP_shift30', 'RECO_shift30', 'CASM_soil_moisture_shift29',\n",
      "       'nmme_wo_ccsm3_nasa', 'nmme0_wo_ccsm3_nasa', 'tmp2m_clim', 'tmp2m_anom',\n",
      "       'tmp2m_shift29', 'tmp2m_shift29_anom', 'tmp2m_shift58',\n",
      "       'tmp2m_shift58_anom', 'mei_shift45', 'phase_shift17',\n",
      "       'sst_2010_1_shift30', 'sst_2010_2_shift30', 'sst_2010_3_shift30',\n",
      "       'icec_2010_1_shift30', 'icec_2010_2_shift30', 'icec_2010_3_shift30',\n",
      "       'wind_hgt_10_2010_1_shift30', 'wind_hgt_10_2010_2_shift30'],\n",
      "      dtype='object')\n",
      "shape (7485181, 26)\n",
      "NaN counts lat                                 0\n",
      "lon                                 0\n",
      "start_date                          0\n",
      "rhum_shift30                    89749\n",
      "pres_shift30                    89749\n",
      "GPP_shift30                   7357469\n",
      "RECO_shift30                  7357469\n",
      "CASM_soil_moisture_shift29    7485181\n",
      "nmme_wo_ccsm3_nasa             653093\n",
      "nmme0_wo_ccsm3_nasa            653093\n",
      "tmp2m_clim                      94889\n",
      "tmp2m_anom                      94889\n",
      "tmp2m_shift29                  104655\n",
      "tmp2m_shift29_anom             109795\n",
      "tmp2m_shift58                  119561\n",
      "tmp2m_shift58_anom             124701\n",
      "mei_shift45                         0\n",
      "phase_shift17                    8738\n",
      "sst_2010_1_shift30             516056\n",
      "sst_2010_2_shift30             516056\n",
      "sst_2010_3_shift30             516056\n",
      "icec_2010_1_shift30            516056\n",
      "icec_2010_2_shift30            516056\n",
      "icec_2010_3_shift30            516056\n",
      "wind_hgt_10_2010_1_shift30          0\n",
      "wind_hgt_10_2010_2_shift30          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#copy of the data and counting the NaN values\n",
    "\n",
    "filtered_df = data\n",
    "#only keeps th integer values, for simplification\n",
    "condition5 = filtered_df['lat'].astype(int) == filtered_df['lat']\n",
    "filtered_df = filtered_df.drop(filtered_df[~condition5].index)\n",
    "condition6 = filtered_df['lon'].astype(int) == filtered_df['lon']\n",
    "filtered_df = filtered_df.drop(filtered_df[~condition6].index)\n",
    "\n",
    "print('columns',filtered_df.columns)\n",
    "# Count NaN values by column\n",
    "print(\"shape\",filtered_df.shape)\n",
    "nan_counts = filtered_df.isna().sum()\n",
    "print('NaN counts', nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddea626-d76d-4fd2-b119-9c9a47b515c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Soil Moisture, GPP, RECO Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1670b840-95e9-4973-bf84-9f67a9388fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_453/4285619449.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_CASM_san['lat'] = df_CASM_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_CASM_san['lon'] = df_CASM_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_CASM_san['lat']=df_CASM_san['lat'].round()\n",
      "/tmp/ipykernel_453/4285619449.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_CASM_san['lon']=df_CASM_san['lon'].round()\n",
      "/tmp/ipykernel_453/4285619449.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_GPP_san['lat'] = df_GPP_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_GPP_san['lon'] = df_GPP_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_GPP_san['lat']=df_GPP_san['lat'].round()\n",
      "/tmp/ipykernel_453/4285619449.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_GPP_san['lon']=df_GPP_san['lon'].round()\n",
      "/tmp/ipykernel_453/4285619449.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RECO_san['lat'] = df_RECO_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RECO_san['lon'] = df_RECO_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_453/4285619449.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RECO_san['lat']=df_RECO_san['lat'].round()\n",
      "/tmp/ipykernel_453/4285619449.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RECO_san['lon']=df_RECO_san['lon'].round()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14348406, 4)\n",
      "lat           0\n",
      "lon           0\n",
      "CASM          0\n",
      "start_date    0\n",
      "dtype: int64\n",
      "(2363760, 4)\n",
      "lat           0\n",
      "lon           0\n",
      "GPP           0\n",
      "start_date    0\n",
      "dtype: int64\n",
      "(2363760, 4)\n",
      "lat           0\n",
      "lon           0\n",
      "RECO          0\n",
      "start_date    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Soil Moisture\n",
    "\n",
    "#creation of a new dataframe based on what has been created in lat_lon_date_data thanks to the \"create_data_matrices\" program.\n",
    "df_CASM=pd.DataFrame()\n",
    "df_CASM['lat']=lat_lon_date_data['lat']\n",
    "df_CASM['lon']=lat_lon_date_data['lon']\n",
    "if target_horizon == \"34w\":\n",
    "    df_CASM['CASM']=lat_lon_date_data['CASM_soil_moisture_shift29']\n",
    "if target_horizon == \"56w\":\n",
    "    df_CASM['CASM']=lat_lon_date_data['CASM_soil_moisture_shift43']\n",
    "df_CASM['start_date']=lat_lon_date_data['start_date']\n",
    "\n",
    "df_CASM_san=df_CASM.dropna(subset=['CASM'])\n",
    "\n",
    "#fill the NaN values with values around\n",
    "df_CASM_san['lat'] = df_CASM_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
    "df_CASM_san['lon'] = df_CASM_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "#Rounds the values to integer\n",
    "df_CASM_san['lat']=df_CASM_san['lat'].round()\n",
    "df_CASM_san['lon']=df_CASM_san['lon'].round()\n",
    "\n",
    "#onnly keeps innteger values andn drop dupliactes\n",
    "condition5 = df_CASM_san['lat'].astype(int) == df_CASM_san['lat']\n",
    "df_CASM_san = df_CASM_san.drop(df_CASM_san[~condition5].index)\n",
    "condition6 = df_CASM_san['lon'].astype(int) == df_CASM_san['lon']\n",
    "df_CASM_san = df_CASM_san.drop(df_CASM_san[~condition6].index)\n",
    "latitude_values = df_CASM_san['lat'].drop_duplicates().sort_values()\n",
    "longitude_values = df_CASM_san['lon'].drop_duplicates().sort_values()\n",
    "\n",
    "### Same process for GPP\n",
    "\n",
    "df_GPP=pd.DataFrame()\n",
    "df_GPP['lat']=lat_lon_date_data['lat']\n",
    "df_GPP['lon']=lat_lon_date_data['lon']\n",
    "if target_horizon == \"34w\":\n",
    "    df_GPP['GPP']=lat_lon_date_data['GPP_shift30']\n",
    "if target_horizon == \"56w\":\n",
    "    df_GPP['GPP']=lat_lon_date_data['GPP_shift44']\n",
    "df_GPP['start_date']=lat_lon_date_data['start_date']\n",
    "\n",
    "df_GPP_san=df_GPP.dropna(subset=['GPP'])\n",
    "\n",
    "df_GPP_san['lat'] = df_GPP_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
    "df_GPP_san['lon'] = df_GPP_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "df_GPP_san['lat']=df_GPP_san['lat'].round()\n",
    "df_GPP_san['lon']=df_GPP_san['lon'].round()\n",
    "\n",
    "condition5 = df_GPP_san['lat'].astype(int) == df_GPP_san['lat']\n",
    "df_GPP_san = df_GPP_san.drop(df_GPP_san[~condition5].index)\n",
    "condition6 = df_GPP_san['lon'].astype(int) == df_GPP_san['lon']\n",
    "df_GPP_san = df_GPP_san.drop(df_GPP_san[~condition6].index)\n",
    "latitude_values = df_GPP_san['lat'].drop_duplicates().sort_values()\n",
    "longitude_values = df_GPP_san['lon'].drop_duplicates().sort_values()\n",
    "\n",
    "\n",
    "### Same process for RECO\n",
    "df_RECO=pd.DataFrame()\n",
    "df_RECO['lat']=lat_lon_date_data['lat']\n",
    "df_RECO['lon']=lat_lon_date_data['lon']\n",
    "if target_horizon == \"34w\":\n",
    "    df_RECO['RECO']=lat_lon_date_data['RECO_shift30']\n",
    "if target_horizon == \"56w\":\n",
    "    df_RECO['RECO']=lat_lon_date_data['RECO_shift44']\n",
    "df_RECO['start_date']=lat_lon_date_data['start_date']\n",
    "\n",
    "df_RECO_san=df_RECO.dropna(subset=['RECO'])\n",
    "\n",
    "df_RECO_san['lat'] = df_RECO_san['lat'].fillna(method='ffill').fillna(method='bfill')\n",
    "df_RECO_san['lon'] = df_RECO_san['lon'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "df_RECO_san['lat']=df_RECO_san['lat'].round()\n",
    "df_RECO_san['lon']=df_RECO_san['lon'].round()\n",
    "\n",
    "condition5 = df_RECO_san['lat'].astype(int) == df_RECO_san['lat']\n",
    "df_RECO_san = df_RECO_san.drop(df_GPP_san[~condition5].index)\n",
    "condition6 = df_RECO_san['lon'].astype(int) == df_RECO_san['lon']\n",
    "df_RECO_san = df_RECO_san.drop(df_RECO_san[~condition6].index)\n",
    "latitude_values = df_RECO_san['lat'].drop_duplicates().sort_values()\n",
    "longitude_values = df_RECO_san['lon'].drop_duplicates().sort_values()\n",
    "\n",
    "## count NaN values in these dataframes \n",
    "print(df_CASM_san.shape)\n",
    "nan_counts = df_CASM_san.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "print(df_GPP_san.shape)\n",
    "nan_counts = df_GPP_san.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "print(df_RECO_san.shape)\n",
    "nan_counts = df_RECO_san.isna().sum()\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08a538e8-ea28-49a6-9ea4-cba995a84386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485181, 29)\n",
      "lat                                 0\n",
      "lon                                 0\n",
      "start_date                          0\n",
      "rhum_shift30                    89749\n",
      "pres_shift30                    89749\n",
      "GPP_shift30                   7357469\n",
      "RECO_shift30                  7357469\n",
      "CASM_soil_moisture_shift29    7485181\n",
      "nmme_wo_ccsm3_nasa             653093\n",
      "nmme0_wo_ccsm3_nasa            653093\n",
      "tmp2m_clim                      94889\n",
      "tmp2m_anom                      94889\n",
      "tmp2m_shift29                  104655\n",
      "tmp2m_shift29_anom             109795\n",
      "tmp2m_shift58                  119561\n",
      "tmp2m_shift58_anom             124701\n",
      "mei_shift45                         0\n",
      "phase_shift17                    8738\n",
      "sst_2010_1_shift30             516056\n",
      "sst_2010_2_shift30             516056\n",
      "sst_2010_3_shift30             516056\n",
      "icec_2010_1_shift30            516056\n",
      "icec_2010_2_shift30            516056\n",
      "icec_2010_3_shift30            516056\n",
      "wind_hgt_10_2010_1_shift30          0\n",
      "wind_hgt_10_2010_2_shift30          0\n",
      "CASM                          6587074\n",
      "GPP                           7352893\n",
      "RECO                          7352893\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_df_CASM = df_CASM_san.groupby(['lat', 'lon', 'start_date'], as_index=False)['CASM'].mean()\n",
    "merged_df = filtered_df.merge(new_df_CASM[['lat', 'lon', 'start_date', 'CASM']], on=['lat', 'lon', 'start_date'], how='left')\n",
    "new_df_GPP = df_GPP_san.groupby(['lat', 'lon', 'start_date'], as_index=False)['GPP'].mean()\n",
    "merged_df = merged_df.merge(new_df_GPP[['lat', 'lon', 'start_date', 'GPP']], on=['lat', 'lon', 'start_date'], how='left')\n",
    "new_df_RECO = df_RECO_san.groupby(['lat', 'lon', 'start_date'], as_index=False)['RECO'].mean()\n",
    "merged_df = merged_df.merge(new_df_RECO[['lat', 'lon', 'start_date', 'RECO']], on=['lat', 'lon', 'start_date'], how='left')\n",
    "filtered_df = merged_df #.drop(['CASM_soil_moisture_shift44 '])\n",
    "\n",
    "print(filtered_df.shape)\n",
    "nan_counts = filtered_df.isna().sum()\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bfd10d-697f-4963-88e1-c3da08051e36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Transform temporal data from Timestamp to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1165e0d9-b63a-4fc3-9338-53673fe17a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# necessary step, timestamp type of date doesn't fit i Pytorch tensors so we have to convert it into integers.\n",
    "filtered_df['start_date'] = filtered_df['start_date'].view('int64').astype(int) #\n",
    "filtered_df = filtered_df.reset_index(drop=True) #line 0 starts now ayt index 0 after modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfec470-420d-44fe-9746-b57c9fe7851e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creation of the Elevation Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f5616a5-59b8-44e8-afbf-ee8b62f9ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lat    lon         elevation\n",
      "0    27.0  236.0    0.000000000000\n",
      "1    27.0  237.0    0.000000000000\n",
      "2    27.0  238.0    0.000000000000\n",
      "3    27.0  239.0    0.000000000000\n",
      "4    27.0  240.0    0.000000000000\n",
      "..    ...    ...               ...\n",
      "708  49.0  262.0  392.001111111111\n",
      "709  49.0  263.0  262.337500000000\n",
      "710  49.0  264.0  328.661805555556\n",
      "711  49.0  265.0  330.006111111111\n",
      "712  49.0  266.0  163.785277777778\n",
      "\n",
      "[713 rows x 3 columns]\n",
      "Elevation loading time= 3.422720193862915\n",
      "lat\n",
      "Elapsed time: 0.049385 seconds.\n",
      "\n",
      "lon\n",
      "Elapsed time: 0.037028 seconds.\n",
      "\n",
      "elevation\n",
      "Elapsed time: 0.037712 seconds.\n",
      "\n",
      "torch.Size([23, 31, 3])\n",
      "Saving multiarrays features to results/matrix/elevation_TEST.tensor\n",
      "Finished generating data matrix.\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "#loads the elevation dataset\n",
    "dataset = xr.open_dataset('new_features/elevationdata.nc')\n",
    "latitudes = dataset['lat'].values\n",
    "latitudes=np.repeat(latitudes,dataset.sizes['lon']) \n",
    "longitudes = dataset['lon'].values\n",
    "elevations=dataset['elevation'].values\n",
    "\n",
    "gt = pd.DataFrame({\n",
    "    'lat': latitudes\n",
    "})\n",
    "longitudes_repeated = np.tile(longitudes, dataset.sizes['lat']) #585\n",
    "df_lon = pd.DataFrame({'lon': longitudes_repeated})\n",
    "elev=[]\n",
    "for lat in elevations:\n",
    "    for lon in lat:\n",
    "        elev.append(lon)\n",
    "df_elev=pd.DataFrame({'elevation':elev})\n",
    "gt=pd.concat([gt,df_lon,df_elev],axis=1)\n",
    "if isinstance(gt.index, pd.MultiIndex):\n",
    "    gt.reset_index(inplace=True)\n",
    "\n",
    "lat_restriction_left = 27 #restriction of data around the US (latitude North)\n",
    "lat_restriction_right = 49 #restriction of data around the US (latitude North)\n",
    "lon_restriction_left = -124 #restriction of data around the US (longitude West)\n",
    "lon_restriction_right = -94 #restriction of data aroud the US (longitude West)\n",
    "\n",
    "#only keep the values inside\n",
    "gt = gt[gt['lat'].between(lat_restriction_left, lat_restriction_right)] \n",
    "gt = gt[gt['lon'].between(lon_restriction_left, lon_restriction_right)]\n",
    "\n",
    "gt['lon'] = np.where(gt['lon']< 0, gt['lon'] + 360, gt['lon'])\n",
    "\n",
    "gt['lat_rounded']=gt['lat'].round()\n",
    "gt['lon_rounded']=gt['lon'].round()\n",
    "\n",
    "gt_new = gt.groupby(['lat_rounded', 'lon_rounded'], as_index=False)['elevation'].mean()\n",
    "\n",
    "elevation_df=gt_new.rename(columns={'lat_rounded':'lat','lon_rounded':'lon'})\n",
    "print(elevation_df)\n",
    "print('Elevation loading time=',time.time()-t)\n",
    "\n",
    "tic()\n",
    "extract_latitudes = elevation_df['lat'].drop_duplicates().sort_values()\n",
    "extract_longitudes = elevation_df['lon'].drop_duplicates().sort_values()\n",
    "extract_latitudes =np.array(extract_latitudes).tolist()\n",
    "extract_longitudes =np.array(extract_longitudes).tolist()\n",
    "extract_features = np.array(elevation_df.columns).tolist()\n",
    "# Create the coordinate map\n",
    "coordinate_map = {}\n",
    "for lat_idx, lat in enumerate(extract_latitudes):\n",
    "    for lon_idx, lon in enumerate(extract_longitudes):\n",
    "        for features_idx, feature in enumerate(extract_features):\n",
    "            coordinate_map[(lat, lon, feature)] = (lat_idx, lon_idx, features_idx)\n",
    "\n",
    "# Create an empty tensor with the desired dimensions\n",
    "tensor_elevation = th.empty((len(extract_latitudes), len(extract_longitudes), len(extract_features)))\n",
    "\n",
    "# Iterate over the dataframe rows and fill the tensor using the coordinate map\n",
    "for columns in elevation_df.columns:\n",
    "    print(columns)\n",
    "    tic()\n",
    "    for _, row in elevation_df.iterrows():\n",
    "        lat, lon, feature = row['lat'], row['lon'], row[columns]\n",
    "        tensor_elevation[coordinate_map[(lat, lon, columns)]] = feature\n",
    "    toc()\n",
    "# Print the filled tensor\n",
    "print(tensor_elevation.shape)\n",
    "\n",
    "cache_dir = os.path.join('results', 'matrix')\n",
    "# e.g., cache_dir = 'results/regression/shared/contest_precip_34w'\n",
    "\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Filenames for data file to be stored in cache_dir\n",
    "data_file = os.path.join(\n",
    "    cache_dir, \"elevation_TEST.tensor\")\n",
    "\n",
    "print(\"Saving multiarrays features to \" + data_file)\n",
    "th.save(tensor_elevation, data_file)\n",
    "\n",
    "print(\"Finished generating data matrix.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54a7ec-ee03-443c-976e-3c713ecaadbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creation of ElNino Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5ca17dd-a31e-4c24-a17a-61dd105e1550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elnino34\n",
      "elnino4\n",
      "0       1979-01-01\n",
      "1       1979-01-02\n",
      "2       1979-01-03\n",
      "3       1979-01-04\n",
      "4       1979-01-05\n",
      "           ...    \n",
      "14383   2018-05-19\n",
      "14384   2018-05-20\n",
      "14385   2018-05-21\n",
      "14386   2018-05-22\n",
      "14387   2018-05-23\n",
      "Name: start_date, Length: 14388, dtype: datetime64[ns]\n",
      "                start_date  El Nino_x  El Nino_y  El Nino\n",
      "0       283996800000000000      24.72      26.41    28.36\n",
      "1       284083200000000000      24.72      26.41    28.36\n",
      "2       284169600000000000      24.72      26.41    28.36\n",
      "3       284256000000000000      24.72      26.41    28.36\n",
      "4       284342400000000000      24.72      26.41    28.36\n",
      "...                    ...        ...        ...      ...\n",
      "14383  1526688000000000000      23.67      27.73    29.07\n",
      "14384  1526774400000000000      23.67      27.73    29.07\n",
      "14385  1526860800000000000      23.67      27.73    29.07\n",
      "14386  1526947200000000000      23.67      27.73    29.07\n",
      "14387  1527033600000000000      23.67      27.73    29.07\n",
      "\n",
      "[14388 rows x 4 columns]\n",
      "start_date\n",
      "Elapsed time: 0.715003 seconds.\n",
      "\n",
      "El Nino_x\n",
      "Elapsed time: 0.710153 seconds.\n",
      "\n",
      "El Nino_y\n",
      "Elapsed time: 0.711960 seconds.\n",
      "\n",
      "El Nino\n",
      "Elapsed time: 0.708499 seconds.\n",
      "\n",
      "torch.Size([14388, 4])\n",
      "Saving multiarrays features to results/matrix/El_NINO_TEST.tensor\n",
      "Finished generating data matrix.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "elnino=['1','34','4']\n",
    "\n",
    "elNino={'elnino1': None, 'elnino34': None, 'elnino4': None}\n",
    "\n",
    "\n",
    "for elnin in elnino:\n",
    "    url = \"https://psl.noaa.gov/data/correlation/nina{}.data\".format(elnin) #El Nino 1\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "\n",
    "    # Split the string into lines\n",
    "    lines = data.split('\\n')\n",
    "\n",
    "    # Remove the first line\n",
    "    lines = lines[32:]\n",
    "\n",
    "    # Remove the last two lines\n",
    "    lines = lines[:-8]\n",
    "\n",
    "    # Join the remaining lines back into a string\n",
    "    modified_data = '\\n'.join(lines)\n",
    "\n",
    "\n",
    "    # Assuming the modified_data variable contains the modified string data\n",
    "    # Split the modified data into lines\n",
    "    lines = modified_data.split('\\n')\n",
    "    # Initialize empty lists for columns\n",
    "    columns = [[] for _ in range(13)]\n",
    "\n",
    "    # Iterate over each line and extract values for each column\n",
    "    for line in lines:\n",
    "        values = line.split(' ')\n",
    "        filtered_values = [value for value in values if value != '']\n",
    "\n",
    "        for i in range(0, 13):  # Extract values for columns 2 to 13\n",
    "            columns[i-1].append(filtered_values[i])\n",
    "\n",
    "    # Create a dictionary of columns\n",
    "    data_dict = {f\"Column {i+2}\": col for i, col in enumerate(columns)}\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    df['Column 14'] = df['Column 14'].str.strip().astype(float)\n",
    "\n",
    "    # Set the index based on the first column (year)\n",
    "    df['Column 14'] = pd.to_datetime(df['Column 14'], format='%Y')\n",
    "\n",
    "    # Convert the index to datetime\n",
    "    df.set_index('Column 14', inplace=True)\n",
    "\n",
    "    for i in range(3,13):\n",
    "        df[f'Column {i}'] = df[f'Column {i}'].replace('', np.nan)\n",
    "        df[f'Column {i}'].str.strip().astype(float)\n",
    "\n",
    "    stacked_df = df.stack()\n",
    "\n",
    "    # Convertir la série résultante en un nouveau dataframe avec une seule colonne\n",
    "    df1 = pd.DataFrame(stacked_df,columns=['El Nino'])\n",
    "    df1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Générer une séquence de dates mensuelles\n",
    "    start_date = '1979-01'\n",
    "    end_date = '2023-12'\n",
    "    idx = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "    # Assiger la séquence de dates comme nouvel index au dataframe\n",
    "    df1.index = idx[:492]\n",
    "    elNino['elnino{}'.format(elnin)]=df1\n",
    "for x in elNino:\n",
    "    df_daily = elNino[x].asfreq('D')\n",
    "    df_daily['El Nino'] = df_daily['El Nino'].ffill()\n",
    "    elNino[x]=df_daily\n",
    "    elNino[x].reset_index(inplace=True)   \n",
    "    elNino[x].rename(columns={'index':'start_date'},inplace=True)\n",
    "    elNino[x]=elNino[x].loc[elNino[x]['start_date'] <= '2018-05-23 00:00:00']\n",
    "    \n",
    "merged_df_elnino=elNino['elnino1']\n",
    "i=0\n",
    "for x in elNino:\n",
    "    if i!=0:\n",
    "        print(x)\n",
    "        merged_df_elnino = pd.merge(merged_df_elnino, elNino[x][['start_date', 'El Nino']], on='start_date', how='left')\n",
    "    i+=1\n",
    "\n",
    "merged_df_elnino['El Nino']=merged_df_elnino['El Nino'].str.strip().astype(float)\n",
    "merged_df_elnino['El Nino_x']=merged_df_elnino['El Nino_x'].str.strip().astype(float)\n",
    "merged_df_elnino['El Nino_y']=merged_df_elnino['El Nino_y'].str.strip().astype(float)\n",
    "merged_df_elnino['start_date'] = merged_df_elnino['start_date'].drop_duplicates()\n",
    "print(merged_df_elnino['start_date'].drop_duplicates())\n",
    "merged_df_elnino['start_date'] = merged_df_elnino['start_date'].view('int64').astype(int) \n",
    "\n",
    "print(merged_df_elnino)\n",
    "\n",
    "#Saves El Nino tensor\n",
    "\n",
    "extract_time = merged_df_elnino['start_date'].drop_duplicates().sort_values()\n",
    "extract_time =np.array(extract_time).tolist()\n",
    "extract_features = np.array(merged_df_elnino.columns).tolist()\n",
    "# Create the coordinate map\n",
    "coordinate_map = {}\n",
    "for time_idx, time in enumerate(extract_time):\n",
    "    for features_idx, feature in enumerate(extract_features):\n",
    "        coordinate_map[(time, feature)] = (time_idx, features_idx)\n",
    "\n",
    "# Create an empty tensor with the desired dimensions\n",
    "tensor_el_nino = th.empty((len(extract_time), len(extract_features)))\n",
    "\n",
    "# Iterate over the dataframe rows and fill the tensor using the coordinate map\n",
    "for columns in merged_df_elnino.columns:\n",
    "    print(columns)\n",
    "    tic()\n",
    "    for _, row in merged_df_elnino.iterrows():\n",
    "        time, feature = row['start_date'], row[columns]\n",
    "        tensor_el_nino[coordinate_map[(time, columns)]] = feature\n",
    "    toc()\n",
    "# Print the filled tensor\n",
    "print(tensor_el_nino.shape)\n",
    "\n",
    "cache_dir = os.path.join('results', 'matrix')\n",
    "# e.g., cache_dir = 'results/regression/shared/contest_precip_34w'\n",
    "\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Filenames for data file to be stored in cache_dir\n",
    "data_file = os.path.join(\n",
    "    cache_dir, \"El_NINO_TEST.tensor\")\n",
    "\n",
    "print(\"Saving multiarrays features to \" + data_file)\n",
    "th.save(tensor_el_nino, data_file)\n",
    "\n",
    "print(\"Finished generating data matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041321c7-f465-4c99-ad13-dbd3adfb5e76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creation of a time tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "888af2f4-2a09-4512-b62d-c09d155551e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving multiarrays features to results/matrix/time_TEST.tensor\n",
      "Finished generating data matrix.\n"
     ]
    }
   ],
   "source": [
    "extract_time = filtered_df['start_date'].drop_duplicates().sort_values()\n",
    "extract_time = th.tensor(extract_time)\n",
    "\n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('results', 'matrix')\n",
    "# e.g., cache_dir = 'results/regression/shared/contest_precip_34w'\n",
    "\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Filenames for data file to be stored in cache_dir\n",
    "data_file = os.path.join(\n",
    "    cache_dir, \"time_TEST.tensor\")\n",
    "\n",
    "print(\"Saving multiarrays features to \" + data_file)\n",
    "th.save(extract_time, data_file)\n",
    "\n",
    "print(\"Finished generating data matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f40598-fb0f-434d-b8e3-474cb302990a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Creation and saving of the final tensor\n",
    "this step is time consuming (about 10 minutes per column), run it during the night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76caca5d-bdb9-44bf-8502-b50c88b15262",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "extract_latitudes = filtered_df['lat'].drop_duplicates().sort_values()\n",
    "extract_longitudes = filtered_df['lon'].drop_duplicates().sort_values()\n",
    "extract_time = filtered_df['start_date'].drop_duplicates().sort_values()\n",
    "extract_latitudes =np.array(extract_latitudes).tolist()\n",
    "extract_longitudes =np.array(extract_longitudes).tolist()\n",
    "extract_time =np.array(extract_time).tolist()\n",
    "extract_features = np.array(filtered_df.columns).tolist()\n",
    "# Create the coordinate map\n",
    "coordinate_map = {}\n",
    "for lat_idx, lat in enumerate(extract_latitudes):\n",
    "    print(lat_idx)\n",
    "    for lon_idx, lon in enumerate(extract_longitudes):\n",
    "        for time_idx, time in enumerate(extract_time):\n",
    "            for features_idx, feature in enumerate(extract_features):\n",
    "                coordinate_map[(lat, lon, time, feature)] = (lat_idx, lon_idx, time_idx, features_idx)\n",
    "\n",
    "toc()\n",
    "\n",
    "#to clear space on the CPU\n",
    "del lat_lon_date_data\n",
    "del date_data\n",
    "\n",
    "# Create an empty tensor with the desired dimensions\n",
    "tensor_3d = th.empty((len(extract_latitudes), len(extract_longitudes), len(extract_time), len(extract_features)))\n",
    "\n",
    "# Iterate over the dataframe rows and fill the tensor using the coordinate map\n",
    "for columns in filtered_df.columns:\n",
    "    print(columns)\n",
    "    tic()\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        lat, lon, time, feature = row['lat'], row['lon'], row['start_date'], row[columns]\n",
    "        tensor_3d[coordinate_map[(lat, lon, time, columns)]] = feature\n",
    "    toc()\n",
    "# Print the filled tensor\n",
    "print(tensor_3d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56ddd5-8654-41cc-871b-63cadad194d7",
   "metadata": {},
   "source": [
    "In the following box, we fill the NaN values using nearest neighbours values. If there is no data at one date, we copy and paste th data of the previous date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a17a1c-40c6-47a8-a882-c23a78dd7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "tensor_filled = reshaped_tensor.clone()  # Create a copy of the tensor\n",
    "# Iterate over each sample in the tensor\n",
    "tic()\n",
    "i=0\n",
    "prev_sample_data = th.zeros(tensor_filled.shape[-3],tensor_filled.shape[-2],tensor_filled.shape[-1])\n",
    "print(prev_sample_data.shape)\n",
    "for sample in tensor_filled: #across the first dimension (temporal)\n",
    "    i+=1\n",
    "    if i%500 ==0: #to follow the code advance\n",
    "        print(i)\n",
    "        #break\n",
    "    for channel in range(sample.shape[-1]): #across the last dimension (features)\n",
    "        # Extract the channel\n",
    "        channel_data = sample[:, :, channel]\n",
    "        nan_count = th.isnan(channel_data).sum().item()\n",
    "        #if nan_count !=0 :\n",
    "            #print(nan_count)\n",
    "        \n",
    "        # Get the non-NaN coordinates\n",
    "        non_nan_indices = np.where(np.isfinite(channel_data))\n",
    "        non_nan_points = np.array(non_nan_indices).T\n",
    "\n",
    "        # Get the NaN coordinates\n",
    "        nan_indices = np.where(np.isnan(channel_data))\n",
    "        nan_points = np.array(nan_indices).T\n",
    "\n",
    "        \n",
    "        #if it exists some non NaN values at this date and for this feature we can interpolate\n",
    "        if len(non_nan_points) > 0: \n",
    "            # Interpolate the NaN values using the non-NaN values\n",
    "            channel_data_nan_filled = griddata(\n",
    "                non_nan_points,\n",
    "                channel_data[non_nan_indices],\n",
    "                nan_points,\n",
    "                method='nearest'  # Choose the interpolation method 'linear', 'nearest', 'cubic'\n",
    "            )\n",
    "            ####we have huge holes in datas, linear and cubic doesn't fill the big holes, only nearest can do it, even if it is less precise\n",
    "       \n",
    "            channel_data_nan_filled = th.from_numpy(channel_data_nan_filled).float()\n",
    "        \n",
    "        \n",
    "            # Replace the NaN values with the interpolated values\n",
    "            channel_data[nan_indices] = channel_data_nan_filled\n",
    "            if nan_count !=0 :\n",
    "                nan_count3 = th.isnan(channel_data).sum().item()\n",
    "                if nan_count3 !=0:\n",
    "                    print(\"Number of NaN values:\", nan_count3)\n",
    "            \n",
    "        ##otherwise we just copy the values of the last time step (another sample) \n",
    "        else :\n",
    "            if prev_sample_data is not None:\n",
    "                #print('1',channel_data.shape)\n",
    "                #print('2',prev_sample_data.shape)\n",
    "                inter2 = prev_sample_data[:,:,channel].clone()\n",
    "                channel_data = inter2\n",
    "                nan_count3 = th.isnan(channel_data).sum().item()\n",
    "                if nan_count3 !=0:\n",
    "                    print(\"Number of NaN values:\", nan_count3)\n",
    "                \n",
    "        intermediate = channel_data.clone()\n",
    "        prev_sample_data[:,:,channel] = intermediate\n",
    "        \n",
    "   \n",
    "toc()\n",
    "# The tensor 'tensor_filled' now contains the inpainted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19531b-f761-46fc-8e62-63824294cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "tensor_filled2 = tensor_filled.clone()\n",
    "# Convert tensor to numpy array\n",
    "for i in range(3,len(tensor_filled[0,0,0,:])):\n",
    "    if i != 90: #useless remove it\n",
    "        \n",
    "        print(i)\n",
    "        array = tensor_filled2[:,:,:,i].numpy()\n",
    "        print(array.shape)\n",
    "####CAUTION : Have to change it and adapt to each column , or find the reason of why some datas are extremely close to 0 or giat. These thresholds only managee tmperature tm2mp problems\n",
    "        #threshold1 = 1e-6  # Adjust the threshold as needed\n",
    "        #threshold2 = 1e4 #careful, these thresholds aare valid for tmp2m but I didn't check elsewhere\n",
    "    # Identify strange values and replace them with NaN\n",
    "        #array[array < threshold1] = np.nan\n",
    "        #array[array > threshold2] = np.nan\n",
    "\n",
    "    # Get the indices of NaN values\n",
    "        nan_indices = np.isnan(array)\n",
    "\n",
    "    # Get the indices of non-NaN values\n",
    "        non_nan_indices = np.argwhere(~nan_indices)\n",
    "\n",
    "    # Create 1D arrays of indices for non-NaN values\n",
    "        x_indices = non_nan_indices[:, 2]\n",
    "        y_indices = non_nan_indices[:, 1]\n",
    "        z_indices = non_nan_indices[:, 0]\n",
    "        values = array[non_nan_indices[:, 0], non_nan_indices[:, 1], non_nan_indices[:, 2]]\n",
    "\n",
    "    # Create a 3D interpolation function using nearest neighbor method\n",
    "        interp_func = interpolate.NearestNDInterpolator((x_indices, y_indices, z_indices), values)\n",
    "\n",
    "    # Get the indices of NaN values\n",
    "        nan_indices = np.isnan(array)\n",
    "\n",
    "    # Get the coordinates of NaN values\n",
    "        nan_coordinates = np.argwhere(nan_indices)\n",
    "\n",
    "    # Fill NaN values with the interpolated values from nearest neighbors\n",
    "        for coord in nan_coordinates:\n",
    "            x, y, z = coord\n",
    "            array[x, y, z] = interp_func(z, y, x)  # Reversed coordinates due to numpy indexing\n",
    "\n",
    "    # Convert the numpy array back to a tensor\n",
    "        tensor_filled2[:,:,:,i] = th.from_numpy(array)\n",
    "        print(tensor_filled2[:,:,:,i])\n",
    "print(tensor_filled2)\n",
    "\n",
    "nan_count = th.isnan(tensor_filled2).sum().item()\n",
    "print(\"Number of NaN values:\", nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86439cd2-3ea4-4f77-8ddf-3251e6610b6d",
   "metadata": {},
   "source": [
    "Saving the tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96646326-2fde-4442-b8c7-319d0cc4e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('results', 'matrix')\n",
    "# e.g., cache_dir = 'results/regression/shared/contest_precip_34w'\n",
    "\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Filenames for data file to be stored in cache_dir\n",
    "if gt_id == \"contest_tmp2m\" :\n",
    "    if target_horizon == \"56w\" :\n",
    "        data_file = os.path.join(cache_dir, \"data_tmp2m56_new.tensor\")\n",
    "    if target_horizon == \"34w\" :\n",
    "        data_file = os.path.join(cache_dir, \"data_tmp2m34_new.tensor\")\n",
    "if gt_id == \"contest_precip\" :\n",
    "    if target_horizon == \"56w\" :\n",
    "        data_file = os.path.join(cache_dir, \"data_precip56_new.tensor\")\n",
    "    if target_horizon == \"34w\" :\n",
    "        data_file = os.path.join(cache_dir, \"data_precip34_new.tensor\")\n",
    "\n",
    "print(\"Saving multiarrays features to \" + data_file)\n",
    "th.save(tensor_filled2, data_file)\n",
    "\n",
    "print(\"Finished generating data matrix.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
